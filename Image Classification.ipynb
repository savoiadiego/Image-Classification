{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ImageClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wztdEjW6Ig87",
        "MdpgVvMsnESm",
        "eiwgJqQInLrG",
        "RgBk69wxnPK8",
        "8dzzqKeTnRpB",
        "0Rk7ekNXnT2G",
        "286FFRZfnbzw",
        "L3yL6WcxpEea"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVngFILtSmv2"
      },
      "source": [
        "# Homework 1 - Image Classification\n",
        "\n",
        "The notebook is divided into several sections:\n",
        "- Setup - Importing libraries, defining the create_csv function, mounting Drive and unzipping the dataset in the proper Drive directory. Indeed, the notebook was created using the Drive integration with Colab, therefore the main directory is the folder /AN2DL/ImageClassification, which was created in advance with the dataset in it.\n",
        "- Preparing the data - The training set and the validation set are prepared, preprocessing images and creating the Datasets objects to be used by the models.\n",
        "- Models:\n",
        "  - First Model*\n",
        "  - Second Model*\n",
        "  - Third Model (VGG-16)*\n",
        "  - Fourth Model (InceptionV3)*\n",
        "  - Fifth Model (ResNet)*\n",
        "- Ensemble Method (Mode of the results) - In this last section, the predictions of the last three models are taken to compute the most predicted class for each test sample.\n",
        "\n",
        "*In each model section, the architecture is defined, the optimization parameters are set, the callbacks are created, the model is trained and finally the predictions on the test set are computed, exporting the results in a csv format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wztdEjW6Ig87"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_1mxoqL0cP1"
      },
      "source": [
        "# Importing the necessary libraries and setting the seed(s) to make the code replicable\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "SEED = 1234\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFHmd-tN1Lhw"
      },
      "source": [
        "# Defining the create_csv function, which will be used to export the prediction results on the test set\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlZBZCAsLEHn"
      },
      "source": [
        "# Mounting Drive to Colab, as the Drive folder /AN2DL/ImageClassification is the main directory for this homework\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeITJJD_1L92"
      },
      "source": [
        "# Unzipping the dataset (named \"MaskDataset.zip\"), which has to be previously put in the homework directory\n",
        "!unzip '/content/drive/My Drive/AN2DL/ImageClassification/MaskDataset.zip'\n",
        "\n",
        "# Saving the directories for the dataset, the training set and the test set (to be used later)\n",
        "cwd = os.getcwd()                                                               # This is the current working directory, in which the dataset has been unzipped\n",
        "dataset_dir = os.path.join(cwd, 'MaskDataset')                                  # This is the dataset directory, which contains the training and the test folders, along with the json\n",
        "training_dir = os.path.join(dataset_dir, 'training')                            # This is the training directory, which contains the training samples\n",
        "test_dir = os.path.join(dataset_dir, 'test')                                    # This is the test directory, which contains the test samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdpgVvMsnESm"
      },
      "source": [
        "# Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMwSzn2JH4XN"
      },
      "source": [
        "# Creating the ImageDataGenerator objects for Training and Validation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Training - Data Augmentation is applied to generate more images for testing the model(s). Additionally, the standard image rescaling is applied.\n",
        "train_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                    width_shift_range=10,\n",
        "                                    height_shift_range=10,\n",
        "                                    zoom_range=0.3,\n",
        "                                    horizontal_flip=True,\n",
        "                                    vertical_flip=True,\n",
        "                                    fill_mode='constant',\n",
        "                                    cval=0)\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Validation - The standard image rescaling is applied.\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u51CWvbw-7Ge"
      },
      "source": [
        "# Setting the batch size, the image shape and creating the Training and Validation generators\n",
        "\n",
        "# Batch size \n",
        "bs = 2\n",
        "\n",
        "# Image shape\n",
        "img_h = 512\n",
        "img_w = 512\n",
        "\n",
        "# Building a dataframe containing all the images in the training folder and their targets from the json\n",
        "import json\n",
        "\n",
        "with open(os.path.join(dataset_dir,\"train_gt.json\")) as f:\n",
        "  targets = json.load(f)                                                        # Loading the target values from the json in a Python dictionary\n",
        "\n",
        "train_dataframe = pd.DataFrame(targets.items())                                 # Converting the items of the dictionary into a Pandas dataframe\n",
        "train_dataframe.rename(columns = {0:'filename', 1:'class'}, inplace = True)     # Renaming the columns of the dataframe\n",
        "train_dataframe[\"class\"]=train_dataframe[\"class\"].astype(str)                   # Converting the class numbers into strings, to be compatible with the \"categorial\" class_mode of the generators\n",
        "np.random.shuffle(train_dataframe.values)                                       # Shuffling the dataframe values\n",
        "\n",
        "valid_dataframe = train_dataframe[4490:]                                        # Taking the last (5614-4490)=1124 images for the validation set (20% of the samples)...\n",
        "train_dataframe = train_dataframe [:4490]                                       # ... and the other 4490 images for the training set\n",
        "\n",
        "# Training Generator\n",
        "train_gen = train_data_gen.flow_from_dataframe(train_dataframe,\n",
        "                                               training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)\n",
        "# Validation Generator\n",
        "valid_gen = valid_data_gen.flow_from_dataframe(valid_dataframe,\n",
        "                                               training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               target_size=(img_h, img_w),\n",
        "                                               class_mode='categorical',\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi8Q90g_I_6X"
      },
      "source": [
        "# Creating the Training and Validation datasets from the previous generators, to be used by the model(s)\n",
        "\n",
        "num_classes = 3                                                                 # The number of classes that we want to classify: 0 (no person wears mask), 1 (all people wear masks), 2 (someone wears mask)\n",
        "\n",
        "# Training Dataset\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# Validation Dataset\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiwgJqQInLrG"
      },
      "source": [
        "# First Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xyjDB2201B7"
      },
      "source": [
        "# This is the same architecture seen in the practical session of the course, but slightly more deeper (depth is 7 instead of 5).\n",
        "# The starting number of filters is the same (8) and it is doubled after every block.\n",
        "# Each block is composed by: a convolutional layer with kernel size 3x3 and stride 1; a ReLU activation function; a MaxPooling layer with pool size 2x2.\n",
        "# Finally, the FC part is composed by the standard Flatten layer, an intermediate layer with 512 units and ReLU activation function, and a SoftMax layer for the output.\n",
        "\n",
        "start_f = 8\n",
        "depth = 7\n",
        "\n",
        "model_1 = tf.keras.Sequential()\n",
        "\n",
        "# Features Extraction\n",
        "for i in range(depth):\n",
        "\n",
        "    if i == 0:\n",
        "        input_shape = [img_h, img_w, 3]\n",
        "    else:\n",
        "        input_shape=[None]\n",
        "\n",
        "    # Conv block: Conv2D -> Activation -> Pooling\n",
        "    model_1.add(tf.keras.layers.Conv2D(filters=start_f, \n",
        "                                       kernel_size=(3, 3),\n",
        "                                       strides=(1, 1),\n",
        "                                       padding='same',\n",
        "                                       input_shape=input_shape))\n",
        "    model_1.add(tf.keras.layers.ReLU())\n",
        "    model_1.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    start_f *= 2\n",
        "    \n",
        "# Classifier\n",
        "model_1.add(tf.keras.layers.Flatten())\n",
        "model_1.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "model_1.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVf_1TfslJb"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model_1.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model_1.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVQE-vYvsw0O"
      },
      "source": [
        "# Optimization parameters\n",
        "# These are the standard ones, using a Categorical CrossEntropy as loss function and the Adam optimizer with a learning rate of 1e-4.\n",
        "\n",
        "# Loss function\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model_1.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEq3Yg0_uIuK"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\n",
        "# The purpose of this piece of code is to create a \"classification_experiments\" folder inside the directory of this homework (if not already created).\n",
        "# Inside it, it creates a folder called \"Model_1_\" followed by the date and the time of execution, to recognize the experiment.\n",
        "# Then, it sets up the callback for the training of the model, saving the model after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\n",
        "# Finally, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\n",
        "\n",
        "# Creating the \"classification_experiments\" folder if not already created\n",
        "exps_dir = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "    \n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "# Creating the folder in which the model will be saved\n",
        "model_name = 'Model_1'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "# Setting up the callback to save the model after each epoch only if there is an improvement in term of validation accuracy\n",
        "callbacks = []\n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir)\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(exp_dir), \n",
        "                                                   monitor='val_accuracy',\n",
        "                                                   mode='max',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,         # It saves the model only if the validation accuracy improves\n",
        "                                                   save_weights_only=False)\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWRfBHQatGCm"
      },
      "source": [
        "# Fitting the model\n",
        "# The training is done on the train_dataset, while the validation is done on the valid_dataset prepared before.\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\n",
        "\n",
        "model_1.fit(x=train_dataset,\n",
        "            epochs=100,  \n",
        "            steps_per_epoch=len(train_gen),\n",
        "            validation_data=valid_dataset,\n",
        "            validation_steps=len(valid_gen), \n",
        "            callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJD2iVev4byt"
      },
      "source": [
        "# Testing the model on the given Test set\n",
        "\n",
        "# If we want to load an already trained model, then we can set load_model to True, otherwise the test will be done on the model trained in the same session\n",
        "load_model = False\n",
        "if load_model:\n",
        "  path = 'Model_1_XXXXXXXXXX'\n",
        "  full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/classification_experiments', path)\n",
        "  test_model_1 = tf.keras.models.load_model(full_path)\n",
        "else:\n",
        "  test_model_1 = model_1\n",
        "\n",
        "# Each image from the test set is prepared and fed to the model for the classification. The output dictionary contains, for each test image, the softmax predictions for each class\n",
        "from PIL import Image\n",
        "\n",
        "image_filenames = next(os.walk(test_dir))[2]                                    # We collect the filenames of the images in the Test set\n",
        "predictions_1 = {}\n",
        "\n",
        "for image_filename in image_filenames:                                          # For every image in the Test set\n",
        "  img = Image.open(os.path.join(test_dir, image_filename)).convert('RGB')       # We open it in an Image variable\n",
        "  img = img.resize((img_h, img_w))                                              # We resize it according to the image shape on which the model is trained\n",
        "  img_array = np.array(img)\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "  prediction = test_model_1.predict(img_array)                                  # We fed it to the model, obtaining the SoftMax probabilities of that image to belong to the three classes\n",
        "  predictions_1[image_filename] = prediction                                    # And we put the result in the dictionary, as the value of the image selected\n",
        "\n",
        "# Then, for each test image, the class with the maximum output value is taken as the predicted class\n",
        "import ntpath\n",
        "\n",
        "results_1 = {}\n",
        "\n",
        "for i in range(0, len(predictions_1)):                                          # For every item in the dictionary created above\n",
        "  image_name = ntpath.basename(image_filenames[i])                              # We get the name of the image, to use it later\n",
        "  pred_class = np.argmax(predictions_1[image_name])                             # We retrieve the class (0, 1 or 2) which has the highest SoftMax probability\n",
        "  results_1[image_name] = str(pred_class)                                       # And we put the class number in the dictionary, as the value of the image selected\n",
        "\n",
        "# Finally, the results (test image, predicted class) are exported in a csv format\n",
        "create_csv(results_1, '/content/drive/My Drive/AN2DL/ImageClassification/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgBk69wxnPK8"
      },
      "source": [
        "# Second Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k03klG2D74Tc"
      },
      "source": [
        "# This is the same architecture seen in the practical session of the course, but with a different number of filters.\n",
        "# Indeed, the initial number of filters is set to 5, and after each block they are quadruplicated instead of doubled.\n",
        "# Each block is composed by: a convolutional layer with kernel size 3x3 and stride 1; a ReLU activation function; a MaxPooling layer with pool size 2x2.\n",
        "# Finally, the FC part is composed by the standard Flatten layer, an intermediate layer with 512 units and ReLU activation function, and a SoftMax layer for the output.\n",
        "\n",
        "start_f = 5\n",
        "depth = 5\n",
        "\n",
        "model_2 = tf.keras.Sequential()\n",
        "\n",
        "# Features Extraction\n",
        "for i in range(depth):\n",
        "\n",
        "    if i == 0:\n",
        "        input_shape = [img_h, img_w, 3]\n",
        "    else:\n",
        "        input_shape=[None]\n",
        "\n",
        "    # Conv block: Conv2D -> Activation -> Pooling\n",
        "    model_2.add(tf.keras.layers.Conv2D(filters=start_f,                  \n",
        "                                     kernel_size=(3, 3),\n",
        "                                     strides=(1, 1),\n",
        "                                     padding='same',\n",
        "                                     input_shape=input_shape))\n",
        "    model_2.add(tf.keras.layers.ReLU())\n",
        "    model_2.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    start_f *= 4\n",
        "\n",
        "# Classifier\n",
        "model_2.add(tf.keras.layers.Flatten())\n",
        "model_2.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "model_2.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a21RAYVd8-oK"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model_2.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model_2.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFMAdWte9ECI"
      },
      "source": [
        "# Optimization parameters\n",
        "# These are the standard ones, using a Categorical CrossEntropy as loss function and the Adam optimizer with a learning rate of 1e-4.\n",
        "\n",
        "# Loss function\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model_2.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pSO8PvBtdR7"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\n",
        "# The purpose of this piece of code is to create a \"classification_experiments\" folder inside the directory of this homework (if not already created).\n",
        "# Inside it, it creates a folder called \"Model_2_\" followed by the date and the time of execution, to recognize the experiment.\n",
        "# Then, it sets up the callback for the training of the model, saving the model after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\n",
        "# Finally, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\n",
        "\n",
        "# Creating the \"classification_experiments\" folder if not already created\n",
        "exps_dir = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "    \n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "# Creating the folder in which the model will be saved\n",
        "model_name = 'Model_2'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "# Setting up the callback to save the model after each epoch only if there is an improvement in term of validation accuracy\n",
        "callbacks_2 = []\n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir)\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(exp_dir), \n",
        "                                                   monitor='val_accuracy',\n",
        "                                                   mode='max',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,         # It saves the model only if the validation accuracy improves\n",
        "                                                   save_weights_only=False)\n",
        "callbacks_2.append(ckpt_callback)\n",
        "\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks_2.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSXUGML39OdZ"
      },
      "source": [
        "# Fitting the model\n",
        "# The training is done on the train_dataset, while the validation is done on the valid_dataset prepared before.\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\n",
        "\n",
        "model_2.fit(x=train_dataset,\n",
        "          epochs=100,  \n",
        "          steps_per_epoch=len(train_gen),     \n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen), \n",
        "          callbacks=callbacks_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2EfkmrE9VT2"
      },
      "source": [
        "# Testing the model on the given Test set\n",
        "\n",
        "# If we want to load an already trained model, then we can set load_model to True, otherwise the test will be done on the model trained in the same session\n",
        "load_model = False\n",
        "if load_model:\n",
        "  path = 'Model_2_XXXXXXXXXX'\n",
        "  full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/classification_experiments', path)\n",
        "  test_model_2 = tf.keras.models.load_model(full_path)\n",
        "else:\n",
        "  test_model_2 = model_2\n",
        "\n",
        "# Each image from the test set is prepared and fed to the model for the classification. The output dictionary contains, for each test image, the softmax predictions for each class\n",
        "from PIL import Image\n",
        "\n",
        "image_filenames = next(os.walk(test_dir))[2]                                    # We collect the filenames of the images in the Test set\n",
        "predictions_2 = {}\n",
        "\n",
        "for image_filename in image_filenames:                                          # For every image in the Test set\n",
        "  img = Image.open(os.path.join(test_dir, image_filename)).convert('RGB')       # We open it in an Image variable\n",
        "  img = img.resize((img_h, img_w))                                              # We resize it according to the image shape on which the model is trained\n",
        "  img_array = np.array(img)\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "  prediction = test_model_2.predict(img_array)                                  # We fed it to the model, obtaining the SoftMax probabilities of that image to belong to the three classes\n",
        "  predictions_2[image_filename] = prediction                                    # And we put the result in the dictionary, as the value of the image selected\n",
        "\n",
        "# Then, for each test image, the class with the maximum output value is taken as the predicted class\n",
        "import ntpath\n",
        "\n",
        "results_2 = {}\n",
        "\n",
        "for i in range(0, len(predictions_2)):                                          # For every item in the dictionary created above\n",
        "  image_name = ntpath.basename(image_filenames[i])                              # We get the name of the image, to use it later\n",
        "  pred_class = np.argmax(predictions_2[image_name])                             # We retrieve the class (0, 1 or 2) which has the highest SoftMax probability\n",
        "  results_2[image_name] = str(pred_class)                                       # And we put the class number in the dictionary, as the value of the image selected\n",
        "\n",
        "# Finally, the results (test image, predicted class) are exported in a csv format\n",
        "create_csv(results_2, '/content/drive/My Drive/AN2DL/ImageClassification/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dzzqKeTnRpB"
      },
      "source": [
        "# Third Model (VGG-16)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRb-rVziPoup"
      },
      "source": [
        "# This model is realized using Transfer Learning with the VGG-16 architecture.\n",
        "# It is imported through the Keras Applications API, using the ImageNet weights and without including the FC part.\n",
        "# Indeed, the FC part is composed by a Flatten layer, two Dense layers (each one with 4096 units) to mimic the original VGG-16 architecture, and a SoftMax layer for the output.\n",
        "# Fine Tuning is applied to the pre-trained network, freezing the imported weight until the 15th layer and training the remaining ones.\n",
        "\n",
        "# Loading the VGG-16 architecture\n",
        "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "\n",
        "# Setting Fine Tuning\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 15 \n",
        "    for layer in vgg.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    vgg.trainable = False\n",
        "\n",
        "# Creating the whole model, adding the FC part after the loaded architecture    \n",
        "model_3 = tf.keras.Sequential()\n",
        "model_3.add(vgg)\n",
        "model_3.add(tf.keras.layers.Flatten())\n",
        "model_3.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
        "model_3.add(tf.keras.layers.Dense(units=4096, activation='relu'))\n",
        "model_3.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1dz0UpOQH_B"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model_3.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model_3.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVoXVK4fQsEo"
      },
      "source": [
        "# Optimization parameters\n",
        "# Since VGG-16 contains a huge number of parameters, here the Stochastic Gradient Descent (SGD) optimizer has been used to avoid being stuck in local minima.\n",
        "# The Nesterov Accelerated Gradient variant has been used to improve the optimizer performance.\n",
        "# The learning rate has been tuned (by trial and error) to a quite small value: 1e-3.\n",
        "\n",
        "# Loss function\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate and optimizer\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr, nesterov=True)\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model_3.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xia-8ZmStpFQ"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\n",
        "# The purpose of this piece of code is to create a \"classification_experiments\" folder inside the directory of this homework (if not already created).\n",
        "# Inside it, it creates a folder called \"Model_3_\" followed by the date and the time of execution, to recognize the experiment.\n",
        "# Then, it sets up the callback for the training of the model, saving the model after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\n",
        "# Finally, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\n",
        "\n",
        "# Creating the \"classification_experiments\" folder if not already created\n",
        "exps_dir = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "    \n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "# Creating the folder in which the model will be saved\n",
        "model_name = 'Model_3'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "# Setting up the callback to save the model after each epoch only if there is an improvement in term of validation accuracy\n",
        "callbacks_3 = []\n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir)\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(exp_dir), \n",
        "                                                   monitor='val_accuracy',\n",
        "                                                   mode='max',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,         # It saves the model only if the validation accuracy improves\n",
        "                                                   save_weights_only=False)\n",
        "callbacks_3.append(ckpt_callback)\n",
        "\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks_3.append(es_callback)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzeFk5ThQyiU"
      },
      "source": [
        "# Fitting the model\n",
        "# The training is done on the train_dataset, while the validation is done on the valid_dataset prepared before.\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\n",
        "\n",
        "model_3.fit(x=train_dataset,\n",
        "          epochs=100,  \n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen), \n",
        "          callbacks=callbacks_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwoVAVOERPbg"
      },
      "source": [
        "# Testing the model on the given Test set\n",
        "\n",
        "# If we want to load an already trained model, then we can set load_model to True, otherwise the test will be done on the model trained in the same session\n",
        "load_model = False\n",
        "if load_model:\n",
        "  path = 'Model_3_XXXXXXXXXX'\n",
        "  full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/classification_experiments', path)\n",
        "  test_model_3 = tf.keras.models.load_model(full_path)\n",
        "else:\n",
        "  test_model_3 = model_3\n",
        "\n",
        "# Each image from the test set is prepared and fed to the model for the classification. The output dictionary contains, for each test image, the softmax predictions for each class\n",
        "from PIL import Image\n",
        "\n",
        "image_filenames = next(os.walk(test_dir))[2]                                            # We collect the filenames of the images in the Test set\n",
        "predictions_3 = {}\n",
        "\n",
        "for image_filename in image_filenames:                                                  # For every image in the Test set\n",
        "  img = Image.open(os.path.join(test_dir, image_filename)).convert('RGB')               # We open it in an Image variable\n",
        "  img = img.resize((img_h, img_w))                                                      # We resize it according to the image shape on which the model is trained\n",
        "  img_array = np.array(img)\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "  img_array = tf.keras.applications.vgg16.preprocess_input(img_array, data_format=None) # We preprocess the image for the specific architecture\n",
        "  prediction = test_model_3.predict(img_array)                                          # We fed it to the model, obtaining the SoftMax probabilities of that image to belong to the three classes\n",
        "  predictions_3[image_filename] = prediction                                            # And we put the result in the dictionary, as the value of the image selected\n",
        "\n",
        "# Then, for each test image, the class with the maximum output value is taken as the predicted class\n",
        "import ntpath\n",
        "\n",
        "results_3 = {}\n",
        "\n",
        "for i in range(0, len(predictions_3)):                                          # For every item in the dictionary created above\n",
        "  image_name = ntpath.basename(image_filenames[i])                              # We get the name of the image, to use it later\n",
        "  pred_class = np.argmax(predictions_3[image_name])                             # We retrieve the class (0, 1 or 2) which has the highest SoftMax probability\n",
        "  results_3[image_name] = str(pred_class)                                       # And we put the class number in the dictionary, as the value of the image selected\n",
        "\n",
        "# Finally, the results (test image, predicted class) are exported in a csv format\n",
        "create_csv(results_3, '/content/drive/My Drive/AN2DL/ImageClassification/')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rk7ekNXnT2G"
      },
      "source": [
        "# Fourth Model (InceptionV3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWnMsn9ZYWtB"
      },
      "source": [
        "# This model is realized using Transfer Learning with the InceptionV3 architecture.\n",
        "# It is imported through the Keras Applications API, using the ImageNet weights and without including the FC part.\n",
        "# Indeed, the FC part is composed by a Flatten layer, one Dense layer with 512 units, and a SoftMax layer for the output.\n",
        "# Fine Tuning is applied to the pre-trained network, freezing the imported weight until the 13th layer and training the remaining ones.\n",
        "\n",
        "# Loading the InceptionV3 architecture\n",
        "inception = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "\n",
        "# Setting Fine Tuning\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 13\n",
        "    for layer in inception.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    inception.trainable = False\n",
        "\n",
        "# Creating the whole model, adding the FC part after the loaded architecture\n",
        "model_4 = tf.keras.Sequential()\n",
        "model_4.add(inception)\n",
        "model_4.add(tf.keras.layers.Flatten())\n",
        "model_4.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "model_4.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN8vVWEuYgmV"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model_4.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model_4.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GtHlEEcYoW1"
      },
      "source": [
        "# Optimization parameters\n",
        "# These are the standard ones, using a Categorical CrossEntropy as loss function and the Adam optimizer with a learning rate of 1e-4.\n",
        "\n",
        "# Loss function\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model_4.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2gnvlufttOb"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\n",
        "# The purpose of this piece of code is to create a \"classification_experiments\" folder inside the directory of this homework (if not already created).\n",
        "# Inside it, it creates a folder called \"Model_4_\" followed by the date and the time of execution, to recognize the experiment.\n",
        "# Then, it sets up the callback for the training of the model, saving the model after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\n",
        "# Finally, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\n",
        "\n",
        "# Creating the \"classification_experiments\" folder if not already created\n",
        "exps_dir = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "    \n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "# Creating the folder in which the model will be saved\n",
        "model_name = 'Model_4'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "# Setting up the callback to save the model after each epoch only if there is an improvement in term of validation accuracy\n",
        "callbacks_4 = []\n",
        "\n",
        "ckpt_dir = os.path.join(exp_dir)\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(exp_dir),\n",
        "                                                   monitor='val_accuracy', \n",
        "                                                   mode='max',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,         # It saves the model only if the validation accuracy improves\n",
        "                                                   save_weights_only=False)\n",
        "callbacks_4.append(ckpt_callback)\n",
        "\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks_4.append(es_callback)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFXIyrv6YtdP"
      },
      "source": [
        "# Fitting the model\n",
        "# The training is done on the train_dataset, while the validation is done on the valid_dataset prepared before.\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\n",
        "\n",
        "model_4.fit(x=train_dataset,\n",
        "          epochs=100,  \n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen), \n",
        "          callbacks=callbacks_4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpx08RmJYx-B"
      },
      "source": [
        "# Testing the model on the given Test set\n",
        "\n",
        "# If we want to load an already trained model, then we can set load_model to True, otherwise the test will be done on the model trained in the same session\n",
        "load_model = False\n",
        "if load_model:\n",
        "  path = 'Model_4_XXXXXXXXXX'\n",
        "  full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/classification_experiments', path)\n",
        "  test_model_4 = tf.keras.models.load_model(full_path)\n",
        "else:\n",
        "  test_model_4 = model_4\n",
        "\n",
        "# Each image from the test set is prepared and fed to the model for the classification. The output dictionary contains, for each test image, the softmax predictions for each class\n",
        "from PIL import Image\n",
        "\n",
        "image_filenames = next(os.walk(test_dir))[2]                                                    # We collect the filenames of the images in the Test set\n",
        "predictions_4 = {}\n",
        "\n",
        "for image_filename in image_filenames:                                                          # For every image in the Test set\n",
        "  img = Image.open(os.path.join(test_dir, image_filename)).convert('RGB')                       # We open it in an Image variable\n",
        "  img = img.resize((img_h, img_w))                                                              # We resize it according to the image shape on which the model is trained\n",
        "  img_array = np.array(img)\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "  img_array = tf.keras.applications.inception_v3.preprocess_input(img_array, data_format=None)  # We preprocess the image for the specific architecture\n",
        "  prediction = test_model_4.predict(img_array)                                                  # We fed it to the model, obtaining the SoftMax probabilities of that image to belong to the three classes\n",
        "  predictions_4[image_filename] = prediction                                                    # And we put the result in the dictionary, as the value of the image selected\n",
        "\n",
        "# Then, for each test image, the class with the maximum output value is taken as the predicted class\n",
        "import ntpath\n",
        "\n",
        "results_4 = {}\n",
        "\n",
        "for i in range(0, len(predictions_4)):                                          # For every item in the dictionary created above\n",
        "  image_name = ntpath.basename(image_filenames[i])                              # We get the name of the image, to use it later\n",
        "  pred_class = np.argmax(predictions_4[image_name])                             # We retrieve the class (0, 1 or 2) which has the highest SoftMax probability\n",
        "  results_4[image_name] = str(pred_class)                                       # And we put the class number in the dictionary, as the value of the image selected\n",
        "\n",
        "# Finally, the results (test image, predicted class) are exported in a csv format\n",
        "create_csv(results_4, '/content/drive/My Drive/AN2DL/ImageClassification/')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "286FFRZfnbzw"
      },
      "source": [
        "# Fifth Model (ResNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaypKdv2mGBS"
      },
      "source": [
        "# This model is realized using Transfer Learning with the ResNet50V2 architecture.\n",
        "# It is imported through the Keras Applications API, using the ImageNet weights and without including the FC part.\n",
        "# Indeed, the FC part is composed by a Flatten layer, one Dense layer with 512 units, and a SoftMax layer for the output.\n",
        "# Fine Tuning is applied to the pre-trained network, freezing the imported weight until the 13th layer and training the remaining ones.\n",
        "\n",
        "# Loading the InceptionV3 architecture\n",
        "resnet = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "\n",
        "# Setting Fine Tuning\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 13\n",
        "    for layer in resnet.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    resnet.trainable = False\n",
        "\n",
        "# Creating the whole model, adding the FC part after the loaded architecture\n",
        "model_5 = tf.keras.Sequential()\n",
        "model_5.add(resnet)\n",
        "model_5.add(tf.keras.layers.Flatten())\n",
        "model_5.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "model_5.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k8ky64-mmZe"
      },
      "source": [
        "# Visualize created model as a table\n",
        "model_5.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model_5.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nr6YT0-mo7e"
      },
      "source": [
        "# Optimization parameters\n",
        "# These are the standard ones, using a Categorical CrossEntropy as loss function and the Adam optimizer with a learning rate of 1e-4.\n",
        "\n",
        "# Loss function\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Learning rate and optimizer\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model_5.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzrRLuSqtxC5"
      },
      "source": [
        "# Setting up the callbacks and Early Stopping\n",
        "# The purpose of this piece of code is to create a \"classification_experiments\" folder inside the directory of this homework (if not already created).\n",
        "# Inside it, it creates a folder called \"Model_5_\" followed by the date and the time of execution, to recognize the experiment.\n",
        "# Then, it sets up the callback for the training of the model, saving the model after each epoch inside the previously mentioned folder, only if the model improved in accuracy on the Validation set.\n",
        "# Finally, Ealy Stopping is also inserted in the callback, to monitor the loss on the Validation set and to stop the training procedure if it becomes worse for \"patience\" steps.\n",
        "\n",
        "# Creating the \"classification_experiments\" folder if not already created\n",
        "exps_dir = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "    \n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "# Creating the folder in which the model will be saved\n",
        "model_name = 'Model_5'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "    \n",
        "# Setting up the callback to save the model after each epoch only if there is an improvement in term of validation accuracy\n",
        "callbacks_5 = []\n",
        "\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(exp_dir), \n",
        "                                                   monitor='val_accuracy',\n",
        "                                                   mode='max',\n",
        "                                                   verbose=0,\n",
        "                                                   save_best_only=True,         # It saves the model only if the validation accuracy improves\n",
        "                                                   save_weights_only=False)\n",
        "callbacks_5.append(ckpt_callback)\n",
        "\n",
        "# Early Stopping is inserted in the callback, stopping the training procedure if the validation loss increases for too long\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    callbacks_5.append(es_callback)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_VXVhz6msSv"
      },
      "source": [
        "# Fitting the model\n",
        "# The training is done on the train_dataset, while the validation is done on the valid_dataset prepared before.\n",
        "# It can go on up to 100 epochs, but the Early Stopping callback explained before allows to stop much earlier.\n",
        "model_5.fit(x=train_dataset,\n",
        "          epochs=100,  \n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen), \n",
        "          callbacks=callbacks_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHypFQiEmw-M"
      },
      "source": [
        "# Testing the model on the given Test set\n",
        "\n",
        "# If we want to load an already trained model, then we can set load_model to True, otherwise the test will be done on the model trained in the same session\n",
        "load_model = False\n",
        "if load_model:\n",
        "  path = 'Model_5_XXXXXXXXXX'\n",
        "  full_path = os.path.join('/content/drive/My Drive/AN2DL/ImageClassification/classification_experiments', path)\n",
        "  test_model_5 = tf.keras.models.load_model(full_path)\n",
        "else:\n",
        "  test_model_5 = model_5\n",
        "\n",
        "# Each image from the test set is prepared and fed to the model for the classification. The output dictionary contains, for each test image, the softmax predictions for each class\n",
        "from PIL import Image\n",
        "\n",
        "image_filenames = next(os.walk(test_dir))[2]                                                # We collect the filenames of the images in the Test set\n",
        "predictions_5 = {}\n",
        "\n",
        "for image_filename in image_filenames:                                                      # For every image in the Test set\n",
        "  img = Image.open(os.path.join(test_dir, image_filename)).convert('RGB')                   # We open it in an Image variable\n",
        "  img = img.resize((img_h, img_w))                                                          # We resize it according to the image shape on which the model is trained\n",
        "  img_array = np.array(img)\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "  img_array = tf.keras.applications.resnet_v2.preprocess_input(img_array, data_format=None) # We preprocess the image for the specific architecture\n",
        "  prediction = test_model_5.predict(img_array)                                              # We fed it to the model, obtaining the SoftMax probabilities of that image to belong to the three classes\n",
        "  predictions_5[image_filename] = prediction                                                # And we put the result in the dictionary, as the value of the image selected\n",
        "\n",
        "# Then, for each test image, the class with the maximum output value is taken as the predicted class\n",
        "import ntpath\n",
        "\n",
        "results_5 = {}\n",
        "\n",
        "for i in range(0, len(predictions_5)):                                          # For every item in the dictionary created above\n",
        "  image_name = ntpath.basename(image_filenames[i])                              # We get the name of the image, to use it later\n",
        "  pred_class = np.argmax(predictions_5[image_name])                             # We retrieve the class (0, 1 or 2) which has the highest SoftMax probability\n",
        "  results_5[image_name] = str(pred_class)                                       # And we put the class number in the dictionary, as the value of the image selected\n",
        "\n",
        "# Finally, the results (test image, predicted class) are exported in a csv format\n",
        "create_csv(results_5, '/content/drive/My Drive/AN2DL/ImageClassification/')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3yL6WcxpEea"
      },
      "source": [
        "# Ensemble Method (Mode of the results)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGj_Y9uwIMFR"
      },
      "source": [
        "# Here, the predictions of the models on the Test set are put together to perform a majority voting.\n",
        "# Given their higher accuracies, this method considers only the predictions made by the last three models (the ones created using a Transfer Learning approach).\n",
        "# The dictionaries containing the predicted class for each image of the Test set are taken as input of the ensemble_majority_voting function.\n",
        "# For each image, the function computes the mode between the predicted classes, taking the class which has been predicted by more models.\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "# Defining the function for the mode computation\n",
        "def ensemble_majority_voting(results_3, results_4, results_5):\n",
        "  modes = {}\n",
        "  keys = results_3.keys()                                                       # We just take number of keys (images) from one, since the Test set is the same in all of them\n",
        "  for k in keys:                                                                # For each image in the dictionaries\n",
        "    array = np.array([results_3[k], results_4[k], results_5[k]])                # The predicted classes are condensed into an array\n",
        "    modes[k] = stats.mode(array)                                                # And the mode between them is taken\n",
        "    \n",
        "  # At this point, the modes have been computed and saved as the values of the corresponding images.\n",
        "  # However, the \"stats.mode(array)\" call, returns a \"Scipy.Mode\" object, and not the single number corresponding to the mode.\n",
        "  # To obtain a clean dictionary composed by the tuples (test image, most predicted class), we need to take that single number.\n",
        "  # This is done by taking, for each image, the first element of the first element of the \"Scipy.Mode\" object \"results\" for each image.\n",
        "  results_mv = {}\n",
        "  keys2 = modes.keys()\n",
        "  for i in keys2:\n",
        "    results_mv[i] = np.array(modes[i][0][0]).tolist()\n",
        "\n",
        "  # Finally, the clean dictionary is returned by the function.\n",
        "  return results_mv"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbBc0TDa4iZc"
      },
      "source": [
        "# The function is called, taking as input the dictionaries containing the predictions made by the last three models.\n",
        "\n",
        "# Calling the function on the actual results returned by the models\n",
        "results = ensemble_majority_voting(results_3, results_4, results_5)\n",
        "\n",
        "# Finally, the results (test image, predicted class) are exported in a csv format\n",
        "create_csv(results, '/content/drive/My Drive/AN2DL/ImageClassification/')"
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}